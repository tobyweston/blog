<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: mocking | bad.robot]]></title>
  <link href="http://baddotrobot.com/blog/categories/mocking/atom.xml" rel="self"/>
  <link href="http://baddotrobot.com/"/>
  <updated>2016-03-30T18:28:43+01:00</updated>
  <id>http://baddotrobot.com/</id>
  <author>
    <name><![CDATA[Toby Weston]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[JMock to Scalamock Cheat Sheet]]></title>
    <link href="http://baddotrobot.com/blog/2015/05/09/jmock-to-scalamock-cheatsheet/"/>
    <updated>2015-05-09T12:41:00+01:00</updated>
    <id>http://baddotrobot.com/blog/2015/05/09/jmock-to-scalamock-cheatsheet</id>
    <content type="html"><![CDATA[<p>An abbreviated set of JMock examples with their Scalamock equivalents.</p>

<!-- more -->


<h2>Mock Objects &amp; the "Context"</h2>

<p>{% codeblock lang:java Mocks and Mockery: Java / JMock %}
private final Mockery context = new JUnit4Mockery();</p>

<p>private final ScheduledExecutorService executor = context.mock(ScheduledExecutorService.class);
private final ScheduledFuture future = context.mock(ScheduledFuture.class);
{% endcodeblock %}</p>

<p>{% codeblock lang:scala Mocks and Mockery: Scala / Scalamock &amp; Specs2 %}
"A test with a mock context in scope" in new MockContext {
  val executor = mock[ScheduledExecutorService]<br/>
  val future = mock[ScheduledFuture[Any]]
  // ...
}
{% endcodeblock %}</p>

<h2>Returns</h2>

<p>{% codeblock lang:java Return a value: Java / JMock %}
context.checking(new Expectations() {{ braces }}</p>

<pre><code>oneOf(executor).shutdownNow(); will(returnValue(asList(waiting)));
oneOf(waiting).cancel(true);
</code></pre>

<p>}});
{% endcodeblock %}</p>

<p>{% codeblock lang:scala Return a value: Scala / Scalamock %}
(executor.shutdownNow <em>).expects().returning(asList(waiting)).once
(waiting.cancel </em>).expects(true).once
{% endcodeblock %}</p>

<p><strong>Notes:</strong></p>

<ul>
<li><code>expects()</code> is required for zero argument method call expectations.</li>
<li>You can leave off <code>once</code>; it will default to the same behaviour</li>
</ul>


<h2>Allowing / Ignoring</h2>

<p>{% codeblock lang:java Allowing: JMock / Java %}
context.checking(new Expectations() {{ braces }}</p>

<pre><code>allowing(executor).scheduleWithFixedDelay(with(any(Runnable.class)), with(any(Long.class)), with(any(Long.class)), with(any(TimeUnit.class))); will(returnValue(future));
oneOf(future).cancel(true);
</code></pre>

<p>}});
{% endcodeblock %}</p>

<p>{% codeblock lang:scala Allowing: Scalamock / Scala %}
(executor.scheduleWithFixedDelay <em>).expects(<em>, </em>, * , *).returning(future)
(future.cancel </em>).expects(true).once
{% endcodeblock %}</p>

<p><strong>Notes:</strong></p>

<ul>
<li>You could also add <code>.anyNumberOfTimes</code> after the <code>returning</code> call but it's unnecessary.</li>
<li>There's no way to distinguish the <em>intention</em> of allowing and ignoring interactions in Scalamock.</li>
</ul>


<h2>Default Values</h2>

<p>JMock will return a default value (as a dynamic proxy) if you set up an expectation but leave off a <code>returnValue</code>. In the example below, we don't care if it returns anything so if the code under test relies on a value, but the test does not, we don't have to express anything in the test.</p>

<p>{% codeblock lang:java Default Values: JMock / Java %}
oneOf(factory).create();
{% endcodeblock %}</p>

<p>If the underlying code were to check, say, that the result of <code>factory.create()</code> was not an empty list with <code>if (result.isEmpty())</code>, JMock would return something sensible and we'd avoid a <code>NullPointerException</code>. You might argue that this side affect should be captured in a test but leaving it off makes the intention of expectation clearer; we only care that <code>create</code> is called, not what it returns.</p>

<p>Scalamock will return <code>null</code> by default. So the above example would give a <code>NullPointerException</code> and you're required to do something like this. Notice we're using a <code>stub</code> and not a <code>mock</code> here.</p>

<p>{% codeblock lang:scala Default Values: Scalamock / Scala %}
val result = stub[Result]
(factory.create _).expects().once.returning(List(result))</p>

<p>{% endcodeblock %}</p>

<h2>Any / Wildcards</h2>

<p>JMock uses <code>with</code> and Hamcrest the matcher <code>IsAnything</code> (<code>any</code>) to match anything. The type is used by the compiler.</p>

<p>{% codeblock lang:java Any: JMock / Java %}
context.checking(new Expectations() {{ braces }}</p>

<pre><code>ignoring(factory).notifyObservers(with(any(SomeException.class)));
oneOf(factory).notifyObservers(with(any(AnotherException.class)));
</code></pre>

<p>}}
{% endcodeblock %}</p>

<p>In the Scala version, use a <a href="http://docs.scala-lang.org/style/types.html#ascription">type ascription</a> to give the compiler a hand in the partially applied method call;</p>

<p>{% codeblock lang:scala Any: Scalamock / Scala %}
(factory.notifyObservers(<em>: SomeException)).expects(*).anyNumberOfTimes
(factory.notifyObservers(</em>: SomeException)).expects(*).once
{% endcodeblock %}</p>

<p><strong>Notes:</strong></p>

<ul>
<li><code>AnotherException</code> is a subtype of <code>SomeException</code> but <code>any</code> will match on literally anything. Using subtypes like this in JMock is a bit of a smell as a test won't fail if a different subtype is thrown at runtime. It may be useful to express intent.</li>
<li><a href="http://stackoverflow.com/questions/30162263/scalamock-wildcard-argument-match-on-subtype">You can't replicate the subtype line</a> in Scalamock; (<code>(factory.notifyObservers(_: AnotherException))</code> doesn't compile.</li>
</ul>


<h2>Throwing Exceptions</h2>

<p>{% codeblock lang:java Exception: JMock / Java %}
final Exception exception = new RuntimeException();
context.checking(new Expectations() {{ braces }}</p>

<pre><code>oneOf(factory).create(); will(throwException(exception));
oneOf(factory).notifyObservers(exception);
</code></pre>

<p>}});
{% endcodeblock %}</p>

<p>{% codeblock lang:scala Exception: Scalamock / Scala %}
val exception = new Exception
(factory.create <em>).expects().throws(exception).once
(factory.notifyObservers(</em>: Exception)).expects(exception).once</p>

<p>{% endcodeblock %}
<strong>Notes:</strong></p>

<ul>
<li>In Scalamock, <code>throws</code> and <code>throwing</code> are interchangeable.</li>
<li>Again, <code>once</code> is optional.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stop Ignoring @Rules]]></title>
    <link href="http://baddotrobot.com/blog/2012/05/05/stop-ignoring-at-rules/"/>
    <updated>2012-05-05T10:50:00+01:00</updated>
    <id>http://baddotrobot.com/blog/2012/05/05/stop-ignoring-at-rules</id>
    <content type="html"><![CDATA[<p>If you're using a version of JMock prior to 2.6.0 and use <code>@RunWith(JMock.class)</code> you may have spotted that your <code>@Rules</code> are actually being ignored when running JUnit tests. This could mean false positives. It's because older versions of the <code>JMock.class</code> extend <code>JUnit4ClassRunner</code> and <code>JUnit4ClassRunner</code> ignores rules.</p>

<p>The good news is that <a href="http://repo1.maven.org/maven2/org/jmock/">JMock 2.6.0</a> and above use the newer <code>BlockJUnit4ClassRunner</code> and this does support rules. Bear this in mind when working with any class and the <code>@RunWith</code> as they may also extend the rule ignoring runner.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Logging is still evil but...]]></title>
    <link href="http://baddotrobot.com/blog/2011/06/22/loggin-is-still-evil-but/"/>
    <updated>2011-06-22T00:00:00+01:00</updated>
    <id>http://baddotrobot.com/blog/2011/06/22/loggin-is-still-evil-but</id>
    <content type="html"><![CDATA[<p>In a <a href="{{%20root_url%20}}/blog/2010/10/18/logging-is-evil-but/">previous post</a>, I was going on about how evil logging is. How it's often confused as a requirement and often badly misused. The upshot of the post was that if you're going to log stuff, in our case using Log4J, lets be honest about it and test it. We should be able to say upfront what's important to log, in what situations and at what log level. Sounds like a straight forward case of test first.</p>

<p>Mocking Log4J however can be a real pain. I've managed it in the past using Apache's logging abstraction and configuring it to use Log4J under the covers but in my previous post, I demonstrated a slightly easier way. A helper class called Log4J that we can use to represent the logging system and that we can make assertions against. Pretty cool so far.</p>

<!-- more -->


<p>There was one caveat, I wasn't entirely happy with the fact that the class would rely on your external Log4J configuration. To assert that a log message appeared at the level INFO for example, you'd have to make sure that the test environment sets up the appropriate class to log at that level. It made for a kind of integration / environmental test which in some cases might be a sensible test but for the most part, I kept seeing test failures down to configuration on different environments. Yuk.</p>

<p>So I updated the helper class to include a log level override which will ignore what the actual configuration says. This means you can write less brittle tests to say things like "ensure my log message is output at debug level regardless of the runtime configuration".</p>

<p>The updated class looks like this.</p>

<p>{% codeblock lang:java %}
public class Log4J {</p>

<pre><code>private final StringWriter writer = new StringWriter();
private final Logger logger;
private final String uuid = UUID.randomUUID().toString();

public static Log4J appendTo(Logger logger) {
    return new Log4J(logger, ALL);
}

public static Log4J appendTo(Logger logger, Level level) {
    return new Log4J(logger, level);
}

private Log4J(Logger logger, Level level) {
    this.logger = logger;
    WriterAppender appender = new WriterAppender(new SimpleLayout(), writer);
    appender.setName(uuid);
    logger.addAppender(appender);
    logger.setLevel(level);
}

public void clean() {
    logger.removeAppender(uuid);
}

public void assertThat(Matcher&lt;String&gt; matcher) {
    org.junit.Assert.assertThat(writer.toString(), matcher);
}
</code></pre>

<p>}
{% endcodeblock %}</p>

<p>Which means you can setup to expect a log level at say the ERROR level like this.</p>

<p>{% codeblock lang:java %}
private final Log4J logger = Log4J.appendTo(Logger.getLogger(Post.class), LogLevel.ERROR);
{% endcodeblock %}</p>

<p>The make assertions like this (which would fail if the matcher fails or because its not logged at the expected level.</p>

<p>{% codeblock lang:java %}
logger.assertThat(containsString(EXCEPTION_MESSAGE));
{% endcodeblock %}</p>

<p>I still think logging is evil and try <em>really</em> hard not to use a single log statement but if you have to, I hope the helper class helps keep you honest in your tests ;) Have a look at the <a href="{{%20root_url%20}}/blog/2010/10/18/logging-is-evil-but/">previous post</a> for more details and extended examples.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Logging is evil but...]]></title>
    <link href="http://baddotrobot.com/blog/2010/10/18/logging-is-evil-but/"/>
    <updated>2010-10-18T00:00:00+01:00</updated>
    <id>http://baddotrobot.com/blog/2010/10/18/logging-is-evil-but</id>
    <content type="html"><![CDATA[<p>Logging is a nightmare. I don't mean here that conveying information about exceptional circumstances is a nightmare, I mean the combination of over eager developers and [<em>insert your current logging framework here</em>] is a recipe for disaster. We've all seen too much of</p>

<p>{% codeblock lang:java %}
Logger log = Logger.getLogger(ThisSucks.class);
...
try {</p>

<pre><code>somethingRisky();
</code></pre>

<p>} catch (SomethingVeryBadException e) {
   log.error(e);
   throw e;
}
{% endcodeblock %}</p>

<p>which is just one example where the exception handling policy for the system (it's a system-wide concern remember) is muddled at best. Nothing is saying that the same exception isn't logged elsewhere or that the exception is even handled correctly or the right people notified. It's not ok to just log and rethrow and every single time we go to declare a new logger, we should think twice.</p>

<!-- more -->


<p>We've taken this very literally in my current project and everyone is actively discouraged from instantiating a logger. I'd rather be explicit that some exception event has occurred and fire an event that some interested party can listen for. This makes perfect sense when you think about the huge log files that someone has to trawl through, armed only with for some vague clue as to what went wrong, a grep manual and the futile hope that developers actually log something useful. All without the context of the code to actually guide them. Good luck.</p>

<p>The disseminated log problem is exacerbated if there is no clear audit trail tying pieces of information together. In a system with thousands of request per second, how do you tie the logged request inputs to some stack trace embedded in the middle of another thousand requests? What's the correlation ID? What should have been a clear set of requirements from the business (in this case, presumably the support team), can easily get lost in the technical translation.</p>

<blockquote><p>Logging is evil, but if you really <em>have</em> to, be honest about it...</p></blockquote>

<p>Asking the business <em>"what information do you want to see in the event of x happening"</em> rather than assuming they want to see some stack trace in a huge log can make a lot of sense. We're often not logging for ourselves (we have debuggers for that), we're often logging for our customers. If we start to think about this stuff early, in terms of exception events and their audience, we can build systems that tell the outside world something meaningful in flexible ways. We start to define a system wide exception handling policy rather than relying of the default exception handler (<code>System.out</code> is rarely the right choice!).</p>

<p>So back to my current project. Logging is evil, but if I really <em>have</em> to log, my saving grace is to be explicit about it. I'll hunt down a customer and I'll write a test to advertise the fact the log contains what they asked for.</p>

<h2>Separating Concerns</h2>

<p>Most of the common logging frameworks make it troublesome to inject a logger instance (for testing). Logging (or preferably, firing an event) should be orthogonal to the classes core behaviour. My preferred approach is the canonical example of using Aspects, or less esoterically, using decorators.</p>

<p>For example, I created a interface <code>Post</code>, to handle HTTP POST requests. Why should I add logging to implementations and open the door to ad-hoc, erratic logging? I shouldn't, but when my implementation <code>CustomerPost</code> requires logging of the request and response, I can decorate with a <code>LoggingPost</code></p>

<p>{% codeblock lang:java %}
public class LoggingPost implements Post {</p>

<pre><code>private static final Logger LOG = Logger.getLogger(Post.class);

public LoggingPost(Post delegate) {
    this.delegate = delegate;
}

@Override
public Response post(Body body) {
   try {
       return delegate.post(body);
   } catch (IOException e) {
       LOG.error(e.getMessage(), e);
       throw e;
   }
}
</code></pre>

<p>}
{% endcodeblock %}</p>

<p>You might be concerned that the try/catch above looks very similar to the original negative example. The good thing about our decorated example above is that by being explicit about this classes responsibility, declaring the usage in the correct context, we can actually define the system wide policy for logging the <code>Post</code> calls in one place, without affecting the contract of the interface. We'd do this for example, on the system boundary, for example where the RESTful API is implemented.</p>

<p>{% codeblock lang:java %}
@Resource
public class CustomerServlet {</p>

<pre><code>public void doPost(Request chuck, Response up) {
    ...
    customer = new LoggingPost(new CustomerPost(...));
    customer.post(...)
    ...
}
</code></pre>

<p>}
{% endcodeblock %}</p>

<h2>Testing the Logger</h2>

<p>In our <code>LoggingPost</code> above, we haven't even tried to inject a logger in to make the testing easier. Instead, mostly because I was being lazy, I used the helper class below. This is intended to represent Log4J in the context of a test and give access to the logger for assertion purposes.</p>

<p>{% codeblock lang:java %}
public class Log4J {</p>

<pre><code>private final StringWriter writer = new StringWriter();
private final Logger logger;
private final String uuid = UUID.randomUUID().toString();

public static Log4J appendTo(Logger logger) {
    return new Log4J(logger);
}

private Log4J(Logger logger) {
    this.logger = logger;
    WriterAppender appender = new WriterAppender(new SimpleLayout(), writer);
    appender.setName(uuid);
    logger.addAppender(appender);
}

public void clean() {
    logger.removeAppender(uuid);
}

public void assertThat(Matcher&lt;String&gt; matcher) {
    org.junit.Assert.assertThat(writer.toString(), matcher);
}
</code></pre>

<p>}
{% endcodeblock %}</p>

<p>Using it in the test for <code>LoggingPost</code> is shown below</p>

<p>{% assign braces = '{{' %}
{% codeblock lang:java %}
@RunWith(JMock.class)
public class LoggingPostTest {</p>

<pre><code>private final Mockery context = new Mockery();
private final Post mock = context.mock(Post.class);
private final Log4J logger = Log4J.appendTo(Logger.getLogger(Post.class));

private static final String EXCEPTION_MESSAGE = "bar bar black sheep...";

@Test
public void shouldDelegate() throws Exception {
    context.checking(new Expectations() {{ braces }}
        one(mock).post(...);
    }});
    new LoggingPost(mock).post(...);
}

@Test
public void shouldLogWhenExceptionIsThrown() {
    try {
        postWill(throwException(new IOException(EXCEPTION_MESSAGE)));
        new LoggingPost(mock).post(...);
        fail();
    } catch (IOException e) {
        logger.assertThat(allOf(containsString("ERROR"), containsString(EXCEPTION_MESSAGE)));
    }
}

@After
public void cleanupLogger() {
    logger.clean();
}

private void postWill(final Action action) {
    context.checking(new Expectations(){{ braces }}
        allowing(mock); will(action);
    }});
}
</code></pre>

<p>}
{% endcodeblock %}</p>

<p>It relies on Log4J's static instances to dynamically add a logger to the list of loggers and thereby appending any generated logs to something that the <code>Log4J</code> test helper can check against. I can't decide if I like this or not. It gives you an extra test that your class under test is using a logger with the name that you expect (<code>"Post.class"</code> in the example above), testing your logger configuration as a by-product.</p>

<p>What I found interesting about this though was that it was always seemed a lot of effort making some logging framework play nicely with mocks, or writing and configuring a custom in memory appender and asserting on it. With the above example, I very quickly added confirmation to existing Log4J infrastructure.</p>

<p>I'd love to hear your comments and how you write tests for logging.</p>

<p>PS. Logging really is evil.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Changing Test Gears]]></title>
    <link href="http://baddotrobot.com/blog/2010/07/09/changing-test-gears/"/>
    <updated>2010-07-09T00:00:00+01:00</updated>
    <id>http://baddotrobot.com/blog/2010/07/09/changing-test-gears</id>
    <content type="html"><![CDATA[<p>Good poker players know when to change gears. They know when to alter their playing style from cautious to aggressive as the game changes and players drop out. They look at how the <em>odds change</em> as the game progresses and react appropriately. It's the same with testing, you gotta know when to change gears.</p>

<p>To put it development terms, good developers know when to change gears. They know when to change their testing style from cautious to aggressive as the code evolves.</p>

<p>Lets pretend there is just three types of testing; <em>unit</em>, <em>integration</em> and <em>acceptance</em>. In the interest of stereotyping, we'll define them simplistically as</p>

<ul>
<li><em>Unit</em> - single object tests, no collaborations (strict I know, but bear with me)</li>
<li><em>Integration</em> - testing object collaborations, for the purposes of this article, lets assume end-to-end testing slot into this bracket</li>
<li><em>Acceptance</em> - leaning towards end-to-end but key here is that they are customer authored. As such, to convince the customer these will likely be relatively coarse grained and start outside the system boundary</li>
</ul>


<!-- more -->


<h2>Starting in a Low Gear with Unit Tests</h2>

<p>People are probably most comfortable with this type of testing. The term unit testing and the technology JUnit have become so intertwined in the Java world, that people often confuse tests written with JUnit as unit tests. They may be, but they may not be. So where's the value in defining the term unit testing and how does knowing what type of test you've just written in JUnit help with changing gears?</p>

<p>Knowing what gear you're in and knowing the terrain that’s coming up is essential for you to select the right gear. Writing a non-unit test in JUnit has value, of course it does, so why should I care if it’s a unit test or an integration test? Knowing where you are and where you want to be is useful because you can defer some things and avoid duplication. So for me, unit testing is good for testing the edge cases (<a href="http://lmgtfy.com/?q=right+bicep+junit+testing">Right BICEP</a>) and exploring the class you're writing.</p>

<p>It can be especially useful when you <em>test drive</em> towards something or explore relationships and your understanding of the classes roles and responsibilities. In this sense, testing becomes a design or analysis activity; a chance to phrase your thinking and understanding in code.</p>

<p>The regression element can quickly lose value here. For example, writing a test with mocks to explore the interaction between two collaborators, A and B. Then writing a separate test to explore the same for classes B and C. Then a test for A, B and C. There was value in each test individually, but is there still value in all three when there is obvious cross-over? When might I consciously choose <em>not</em> to write unit tests?</p>

<h2>Changing up to Integration Tests</h2>

<p>If I know the context that a set of objects are going to work together in, I'm going to want to be confident that they work together as expected. I can't test these in isolation, so I'm going to need to test them in cooperation. At this level, my confidence is fairly high around the composites so I'm already up and running. I'm more concerned here with a broader brush approach. I'm certainly not interested in re-testing all the lower level object tests, just how they operate together.</p>

<p>So I change up a gear and as a developer, convince myself that these object work together <em>in context</em>. I'm most likely still using JUnit but I do so with a clear understanding of what gear I'm in.</p>

<h2>Cruising with Acceptance Tests</h2>

<p>So how about the value to the business? The unit tests in particular don't advertise value to the business, they're a developer tool and its all too easy to write individual classes well with good test coverage and yet combine them into something that doesn't work for the business.</p>

<p>Demonstrating to business that their specifications have been meet is the ultimate gear, and to change up to that gear and have supreme confidence in the system means going through the previous gears. Knowing that you'll be changing up whilst in lower gears can help you decide what to do (and what to leave out) in those lower gears.</p>

<p>For example, lets assume the business want to <em>see</em> the affect of a configuration file in the system. When developing the code to load and decode the contents of the file, you started by proving the component works at the unit level so why would you test that the component is wired up correctly as a unit test? You'd be forced to mock that component in some higher level component and test it calls it. But what does this give you? You can still wire the higher level component up incorrectly in production. The acceptance test is going to have to test this to demonstrate the affect, so there's an argument to say you can leave it to the acceptance test to verify.</p>

<h2>The Missing Gear?</h2>

<p>When we build software in a componentised way, we're often left with objects that work in isolation with their dependencies passed in. We push up and up the assembly of the objects and are left with parts of the system that are responsible for this assembly. This might be done in code or by some dependency injection framework. I'm really talking about the <em>configuration</em> of your application here, be it done declaratively via Spring's application context or imperatively in code.</p>

<p>Either way, it feels like these 'assemblers' have a clear role and responsibility and so shouldn't they be tested? What gear do we test them in? Personally, I'm comfortable in limiting the assembly options and so reduce the combinations of testing required. With a single "builder" or "spring context", I'm comfortable with testing these through acceptance tests.</p>

<p>In this article, I suppose I'm using "gears" as an analogy for pragmatism and certainly not pace. I'm not saying that good developers know when to rush, compromising quality but they do know how to optimise their testing strategies. I think its important not to get bogged down in exhaustive unit-style testing if its not of value. Understanding what gear you're in and what gear you'll soon be in can help focus your attentions and avoid retesting the same thing again and again.</p>

<p>"What to test and when to test it" is a question I find myself asking again and again.</p>
]]></content>
  </entry>
  
</feed>
