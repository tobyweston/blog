<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: agile | bad.robot]]></title>
  <link href="http://baddotrobot.com/blog/categories/agile/atom.xml" rel="self"/>
  <link href="http://baddotrobot.com/"/>
  <updated>2016-03-23T20:54:47+00:00</updated>
  <id>http://baddotrobot.com/</id>
  <author>
    <name><![CDATA[Toby Weston]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Daily Standups Don't Work]]></title>
    <link href="http://baddotrobot.com/blog/2012/09/15/daily-standups-dont-work/"/>
    <updated>2012-09-15T06:12:00+01:00</updated>
    <id>http://baddotrobot.com/blog/2012/09/15/daily-standups-dont-work</id>
    <content type="html"><![CDATA[<p>At some point, standups have stopped working for me. They've certainly moved away from the original intention to improve collaboration and communication. I'm not sure I can put my finger on why, but I'm just not getting much out of them any more. It's led me to think that standups per se just don't work. At least in most of the environments I've encountered. So I've been thinking about what could work in their place and I think its just to <em>talk</em> more.</p>

<!-- more -->


<h2>Typical Problems</h2>

<p>Some typical problems I see again and again include standups taking too long; standups becoming a tool to chase progress or apply pressure and attendees glazing over when it's not their turn to speak. <a href="http://jchyip.blogspot.co.uk/">Jason Yip</a> talks about some other <a href="http://martinfowler.com/articles/itsNotJustStandingUp.html#HowDoWeKnowWhenAStand-upIsGoingPoorly">common problems</a>. The biggest problem that I see though, is that standups have become more about the "status update" than communication and collaboration.</p>

<h2>It's Not A Status Update</h2>

<p>If your standup is more about the status update than anything else, the rot may have already set in. It's a shame  the first sentence of the <a href="https://en.wikipedia.org/wiki/Stand-up_meeting">wikipedia page</a> defines a standup as being a "status update". In fact, nearly every page on the subject talks about it in terms of a "status update". Nonsense.</p>

<p>In my view, it should be about communication and collaboration. The trouble with "status update" as a phrase, is that it has dark undertones. It has <em>reporting</em> connotations and can promote a command and control relationship. It's pretty common for team leads, project managers or other stakeholders to coerce the standup into a tool for reporting. <strong>That's what the board is for.</strong></p>

<p>When it becomes a tool to apply pressure or push a project management agenda, things can get pretty negative. Good team leads, managers and stakeholders will use the board for status updates and go round the team individually as they need to. It's all too easy to condense this into an intense experience at standup but it won't tell them the whole story. Those in project management roles have to work hard to preserve the spirit of the standup and get management information in other ways.</p>

<h2>Communication &amp; Collaboration</h2>

<p>If the spirit of the standup is really about communication and collaboration, why don't we apply the same principles we apply with Extreme Programming, namely, to apply it all the time? <strong>I'd prefer standups to be more organic.</strong> Why not jump up and start a conversation when you feel like it? Grab anyone who looks up. Why prescribe a meeting first thing, when you may not have anything to talk about? When you're working in the same physical proximity, it's natural to overhear and contribute to the conversations around you. Standup, get involved.</p>

<p>The standard <a href="http://www.extremeprogramming.org/rules/standupmeeting.html">three report items</a> feel a bit arbitrary and anyway, they're really intended as a guide, not a mantra. I'd go further than that and suggest that even the idea of <em>reporting</em> at standup is the thin end of the wedge. The three questions promote the idea of <em>reporting</em> when we should be promoting the idea of <em>collaboration</em>.</p>

<p>In the same way, wouldn't it be nice if you pair with whomever, whenever the time is right? Rather than move board avatars around in the morning at standup to organising pairs, wouldn't it be nice to seek a collaborator as you need to and change them often? A bit like socks.</p>

<h2>The Good Bits</h2>

<p>Of course, none of this may be ringing true for you. Standups might be the perfect forum for communication for your team. Despite the title, I don't mean to suggest it plainly never works. I just want to emphasise how difficult it is to get it to work and avoid the pitfalls. There may even be a simpler, less leading mechanisms to promote communication and keep team focus. I'm talking here about <em>natural</em> conversation and social cohesion.</p>

<p>To offer at least a token effort at balance; even in a dysfunctional standup, I can still find a few things useful. I do like to start the day with a focusing session. A bit like <a href="/blog/2012/07/20/getting-things-done-i">GTD</a> where you ask "what is my immediate next action?". It can also be a convenient time to pair up for the day and unfortunately, it does work as a status update tool for management.</p>

<h2>That's All Folks</h2>

<p>It's down to individuals in the team to engage. Forcing a standup meeting isn't going to do that. Individuals should be nosey, keeping an eye on the board and the backlog. They don't need to stand in front of the board and watch cards go up or move to the right in a daily standup to do that. They don't need to listen to a synopsis of yesterday's work if they were listening to it unfold yesterday. Management need another forum if they're hijacking your standup and as an engaged individual, it's up to you to champion that change.</p>

<p>Like a lot of the agile practices, its easy to fall into the habit of the daily standup without stopping to consider why we're doing so. If you really don't think you're getting much out of it, stop and question things. Following agile practices mechanically isn't the goal in itself; it's about more than that.</p>

<p>If you fancy shaking things up, <a href="http://www.planningforfailure.com/">Todd Charron</a>'s created an interactive course with practical ideas to improve your standup. <a href="http://www.udemy.com/improv-your-agile-scrum-stand-up?couponCode=pffcommented">Check it out</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pair Tests: What Candidates Can Expect]]></title>
    <link href="http://baddotrobot.com/blog/2012/07/04/pair-tests-what-candidates-can-expect/"/>
    <updated>2012-07-04T18:58:00+01:00</updated>
    <id>http://baddotrobot.com/blog/2012/07/04/pair-tests-what-candidates-can-expect</id>
    <content type="html"><![CDATA[<p><img itemprop="image" class="right" src="../../../../../images/pairing.jpg"></p>

<p>If you haven't had the opportunity to pair much in your day job, it may seem a little unfair to be <em>tested</em> on the subject. More and more, people are trying to recreate working environments in order to assess candidates and that means <em>pairing</em>. The trouble is, <em>pair tests</em> are often not an accurate recreation and it takes some skill from both candidate and interviewer to get through a pair test smoothly.</p>

<p>This post talks a little about what to expect from a pair test and offers a few tips to surviving the process.</p>

<!-- more -->


<h2>What To Expect</h2>

<p>Typically, you'll sit down with one or two developers, be introduced to a basic problem and asked to solve it. If it's a good sample problem, you can expect to be asked to implement or fix small, incremental pieces of functionality. It's unfortunate that we use the term "pair test". I don't see it as a "test" but as an exercise or discussion. There shouldn't be a single right or wrong answer. The prospective employer should really be looking at <em>how you work</em> rather than <em>did she solve the problem?</em> A good <em>coding exercise</em> is simple and open ended.</p>

<p>Expect to be put at ease. The interviewer should make every effort to relax you and communicate their expectations. If you find that your dropped in the deep end and bombarded with all kinds of devious scenarios, you have to question the organisation's values.</p>

<p>Expect a choice of IDE. If you're not given a choice, don't expect to be judged on your knowledge of the IDE.</p>

<p>Expect to be coding for an hour to two. They inevitably take a while and it's unrealistic to expect a productive pairing session in just thirty minutes.</p>

<p>After the exercise, you may be asked "how did you find that?". If not, I like to ask the interviewer "how was that? Was it the kind of thing you were looking for?". It may lead to further conversations that give you the chance to talk about things in more depth, discuss alternatives and generally impress.</p>

<h2>Be Yourself</h2>

<p>There's often an unspoken expectation that you'll be demonstrating your mad TDD skills. I think deep down though, people are interested in seeing how you work, not necessarily that you work in a test driven way. I think it's fair to say people do expect to see some tests, so make sure you write <em>some</em>. My point is that you shouldn't be penalised if you right them retrospectively rather than first. If you're not comfortable doing TDD in your day job, don't suddenly make out it's your bread and butter. Try and work how you would on your own. Don't fake it.</p>

<p>Another motivation for the pairing exercise is to see if, as a prospective pair, you can get on. If you relax and let your personality come through, you'll both have a better idea what it would be like pairing day in, day out.</p>

<h2>Hang ups</h2>

<p>Don't get hung up on the IDE, the API or even the problem. The interviewer shouldn't be judging you based on what shortcuts your know or what API you know off by heart. If they do, then you should question what's important to them; an adaptive, bright, enthusiastic developer or a robot that knows emacs keyboard bindings?</p>

<p>With regards to the problem; if its not obvious, ask. A well set pairing exercise should be simple enough to quickly understand what's required and get you coding. Bogging you down with the complexities of the problem doesn't help anyone. Really. Don't be afraid to ask, it's what we do in our day jobs right?</p>

<h2>Nerves</h2>

<p>The thing that always gets me is nerves. I put way too much pressure on myself with an involuntary, internal commentary saying "am I doing enough? is that what they want to see? do they want to see this fancy thing? or that fancy thing?". Don't be like me, give yourself a break. You're a professional and good at what you do, you don't need to show off.</p>

<p>I guess this all leads to my final point...</p>

<h2>Summary</h2>

<p>Discuss the problem. Discuss what you're thinking. Discuss what you plan to do. Discuss. Discuss. Discuss.</p>

<p>If you get the chance, even mention some of the awesome things you've done. It's a great opportunity to talk to the devs you could actually be working with. So ask them about the project, about what floats <em>their</em> boat. Remember that you're interviewing them as much as the other way round.</p>

<p>Communication is key in what we do and it's key in any pairing exercises. If you're open, honest and communicative, you won't go far wrong in a well conducted pairing exercise. Don't forget too that conducting a pairing exercise is difficult and takes practice. So don't be too disheartened if it doesn't go well, but <a href="/blog/2011/08/29/reflecting-on-interviewing-mistakes">reflect and learn from it</a>.</p>

<p>Good luck!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hexagonal Acceptance Testing]]></title>
    <link href="http://baddotrobot.com/blog/2012/02/13/hexagonal-acceptance-testing/"/>
    <updated>2012-02-13T21:24:00+00:00</updated>
    <id>http://baddotrobot.com/blog/2012/02/13/hexagonal-acceptance-testing</id>
    <content type="html"><![CDATA[<p>There's no reason a unit test can't also be an acceptance test. If you can prove that the <em>unit</em> behaves a certain way <em>and</em> that in production, it will behave in the same way as in your unit test, the intersection should give you enough confidence.</p>

<p>What seems to make things harder to discuss is that its difficult to agree on a common definition for the different types of testing. As a peer-group, we're usually horrified by the previous paragraph and ask the question <em>"shouldn't we start up the entire stack in an acceptance test?"</em>. This often leads us to long-running, duplicative and expensive tests in the name of acceptance testing.</p>

<p>Taking inspiration from Cockburn's <a href="http://alistair.cockburn.us/Hexagonal+architecture">Hexagonal Architecture</a> and being more flexible in our technical definitions of acceptance testing however, we <em>can</em> create lightning fast acceptance testing.</p>

<!-- more -->


<p></p>

<h2>Conway's Law</h2>

<p>To paraphrase something <a href="https://twitter.com/#!/Jazzatola">@Jazzatola</a> recently said</p>

<p><blockquote><p>"people are usually happy to test interactions with other systems 'by specification' but are less happy to do so when testing their internal systems"</p><footer><strong>@Jazzatola</strong> <cite><a href='https://twitter.com/#!/Jazzatola'>twitter.com/#!/Jazzatola/&hellip;</a></cite></footer></blockquote></p>

<p>As he points out; we're happy to say "given the external system responds with <code>X</code>, when we send a message <code>Y</code> then our system behaves <code>Z</code>". We know the API and test against it as a <em>specification</em>, typically via <em>mocking</em> the behaviour of the external system and testing against the response. We're less happy to talk about our internal interactions as internal APIs in the same way.</p>

<p>I find this interesting for a couple of reasons. <a href="https://twitter.com/#!/Jazzatola">@Jazzatola</a> was suggesting that this is an example of <a href="http://en.wikipedia.org/wiki/Conway's_law">Conway's law</a>; where the communication structures within an organisation are leading design. We're physically separated from our external system actors but intimately acquainted with the internal communication flows. After all, we wrote them.</p>

<p>It's also interesting because it can limit how we go about implementing our acceptance tests.</p>

<h2>Hexagonal Implementation</h2>

<p>If we look at our system as a series of <em>ports</em> and <em>adaptors</em> (as in Cockburn's <a href="http://alistair.cockburn.us/Hexagonal+architecture">Hexagonal Architecture</a>), we can start to test it as a series of internal, co-operating handoffs. Don't we already test our systems like this; with conventional mocking? The difference is that this kind of mocking is at a finer grained level; we mock <em>collaborators</em> to create unit-style tests and drive out design. When we mock external systems and the <em>ports</em> in our internal systems, we're mocking coarse grained <em>behaviours</em>. We're confirming an established design rather than driving one out. We can formalise this established design as <em>ports</em>.</p>

<p>If you look closely at the last few sentences you'll notice that I'm talking about <em>test confirm</em> at the coarse grained level rather than the <em>test driving</em> techniques we apply with unit-style tests. I think this is natural fit for acceptance testing where we should be thinking about testing the external affects produced by internal flows (more black than white box).</p>

<p>So, given we expect an internal interaction to behave in such-and-such way, why do we need to startup the entire application to exercise the effect of that behaviour? We don't. We can <em>simulate</em> the specification internally by mocking and produce a series of overlapping tests. Each one supporting and giving more confidence to the last.</p>

<p>We've certainly had some great successes with this technique. We've produced faster running test suites that customers were happy to "accept" or sign-off against. We built trust working with the customers to understand the approach and put ourselves more easily in their 'voice' describing the system as a series of internal API interactions.</p>

<h3>Conventional Hexagonal Architecture Footnote</h3>

<p>In the original article, Cockburn talks about ports and adaptors as a fairly abstract architectural approach. He describes it in terms of a pattern which have slightly different motivations than those described here.</p>

<p>He talks about a relatively small number of <em>ports</em> (say ~4-5) and decoupling <em>major</em> components of a system (for example, the database and the GUI) so that it can be driven and tested by different <em>external</em> actors.</p>

<p>I'm talking more about changing the <em>external</em> nature of these actors to be more <em>internal</em>. If we have a much larger number of ports (say >30), decoupling <em>minor</em> components we can achieve this substitutabiliy on a bigger scale and be more flexible on how we test the system.</p>

<p>Cockburn described an approach where we can test the system at end-to-end via it's ports. Taking this further to confirm small business functionality or <em>acceptance criteria</em> is a logical progression and fits nicely into iterative development.</p>

<p>Have a go and see if it works for you...</p>

<p><div class="wikipedia">
  
  <h2>Conway's law</h2>
  <div>
    <p><strong>Conway's law</strong> is an <a href="http://en.wikipedia.org/wiki/Adage" title="Adage">adage</a> named after <a href="http://en.wikipedia.org/wiki/Computer" title="Computer">computer</a> <a href="http://en.wikipedia.org/wiki/Programmer" title="Programmer">programmer</a> <a href="http://en.wikipedia.org/wiki/Melvin_Conway" title="Melvin Conway">Melvin Conway</a>, who introduced the idea in 1968. It states that "<i>organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations</i>". </p>
  </div>
  <a href="http://en.wikipedia.org/wiki/Conways_Law" target="_blank">Read more on Wikipedia</a>
</div></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Reflecting on Interviewing Mistakes]]></title>
    <link href="http://baddotrobot.com/blog/2011/08/29/reflecting-on-interviewing-mistakes/"/>
    <updated>2011-08-29T00:00:00+01:00</updated>
    <id>http://baddotrobot.com/blog/2011/08/29/reflecting-on-interviewing-mistakes</id>
    <content type="html"><![CDATA[<p>Recruiting for the next guy on your team is hard. At first glance it doesn't seem to be, we've developed techniques like pair tests but as I start to look at it more closely, I've started to notice that even the more progressive techniques don't preclude us from making the same mistakes as the traditional interview.</p>

<p>Let's take an example from two teams.</p>

<!-- more -->


<p>Team A's process starts off by favoring buzz word heavy CVs and CVs that meet a minimum number of years of experience. A unattended pen and paper test, characterised by very closed questioning against specialist areas of the programming language. This might include questions around language syntax semantics (keywords and modifiers, object equality etc etc). Things like bubble sorts algorithms are requested. Scores out of 100 are tallied. Things are fairly black and white.</p>

<p><span class='pullquote-right' data-pullquote='but often an implied hurdle that the candidate must jump is &#8220;has he reached the same conclusion as me on topic X?&#8221; '>
Team B's process favors mention of agile experience in the CV. Follow up questions prompt genuine conversation but often an implied hurdle that the candidate must jump is "has he reached the same conclusion as me on topic X?". The unattended coding exercise is not a test, at least it should be more of an exercise to explore the way a candidate approaches things. The team might require the presence of unit tests and evidence of TDD but should actively not persecute style or syntax. Something that's harder in practice to do than in theory.
 </span></p>

<p>Hopefully, its clear that Team A's selection process is heavily biased towards developers with good memories. It's probably unfairly prejudice against candidates that haven't had specific exposure to specific scenarios / solutions. I experienced this when I was asked to write a algorithm to calculate prime numbers with pen and paper. I fumbled through and handed over my scrawl. I explained that I'd prefer write tests, experiment with the code and improve the design; basically to learn as I went along. The response from the interviewer, looking down at my scribbling, was "that's not really what we're looking for... have you heard of the Sieve of Eratosthenes?". Obviously, I hadn't.</p>

<p>Rather than assess my approach, the interviewer was looking for a specific piece of knowledge but what for? If I got the job I'm pretty sure my first task wouldn't be to write something to work out prime numbers. Would that fix some production problem? Would it introduce a new feature that had no other solution? No.</p>

<p>A huge part of what we do is learn, or at least it should be. Failure is what makes us better and in environments where failure is embraced and we write code that we can (fairly) easily rework, we get better systems (as we refine our understanding). We never now what the real problems are going to be when we start a story. The interviewer above simply brushed over this, it seemed he wanted me to reach the same conclusion he had without explaining the steps I took to get there. Without any advocacy on my part, how would he know I could do it again with a different problem?</p>

<blockquote><p>"Right or wrong answers don't really have a place because there's never a right or wrong answer in what we do."</p></blockquote>

<p>Having said all that, I'm sure we'd all favour a process like Team Bs but I'm starting to see that Team B are making at least some of the same mistakes just in a more subtle way...</p>

<p>For the CV selection, Team A look for "spring", "hibernate" and other technology buzzwords. Team B look for "refactoring", "TDD", "XP" and other development buzz words, the reason usually cited as being because the technologies aren't as import. Team B are favouring the <em>why</em> over the <em>how</em>, they're assuming given the right approach and smart people, specifics around technologies can be learnt. Both teams are trying to expose characteristics of the candidates that mirror their own.</p>

<p>Team B asks candidates to complete a short programming exercise off-line. Implement a library, a DVD store, a robot explorer, whatever. It should only take an hour or so and demonstrates the candidates style. I've certainly seen it as an effective tool to eliminate people that really can't code for toffee but I've also seen people fall into the same old trap and eliminate people who missed something specific hidden there. A trivial example might be "oh! they didn't use dependency injection. Fail!".</p>

<p>Team B's pair test should be a great way to understand how a candidate operates in front of an IDE and if you'll actually be able to work with him. A bit like the unattended test, it's a good way to eliminate extreme cases. If the candidate behaves completely anti-socially, wont listen and codes like mad man, you can probably reject him with confidence. It's easy to let bad interview habits creep in though; to focus more on some obscure gotcha in the code than how the candidate is actually pairing.</p>

<blockquote><p>"I think the problem with both these techniques (unattended exercise and the pair test) is when too much specificity comes in at the start. When you are looking for something specific, you'll often be disappointed."</p></blockquote>

<p>I've certainly heard myself say "oh, he didn't spot that there was a precision issue with double there...". In all honesty, I'd miss that kind of bug as often as I'd spot it but I'd hire me! The upshot there, especially when we doing a couple of pair tests a week, is to stay focused on why you're doing the pair test and not on the test itself. Are we doing this to see if the candidate can spot all the traps and pitfalls that we spent so long putting in or do we want to see how they pair? In my view, if they get the "right" answer is almost irrelevant, it's how they explore the problem.</p>

<p>I guess what I'm reflecting on here is how as a peer group, we pretty much realise that closed questioning limits our choices and that open ended questions lead to real conversations that are more relevant to the types of conversations we have day to day. Right or wrong answers don't really have a place because there's never a right or wrong answer in what we do. If I implement a prime number finder without the Colander of Eratosthenes, am I wrong? The tests still pass so I must be right? Is Eratosthenes more right? Despite this realisation though, we can easily fall into a more subtle way of behaving where we mentally start ticking off specifics for a candidate.</p>

<p>I guess we have to keep reminding ourselves what's important and what we're looking for in a candidate. I guess I'm mellowing in the way I assess candidates and probably rejected a fair few unfairly in the past. Sorry.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pairing Honestly]]></title>
    <link href="http://baddotrobot.com/blog/2010/08/15/pairing-honestly/"/>
    <updated>2010-08-15T00:00:00+01:00</updated>
    <id>http://baddotrobot.com/blog/2010/08/15/pairing-honestly</id>
    <content type="html"><![CDATA[<p>Recently we had particularly good retrospective where the team were able to admit that each of us has had difference experiences pairing. We were honest in saying that despite having "done pairing" we'd all done different amounts of pairing and that, at times, we weren't even sure what we were supposed to get out of it.</p>

<p>There can be a fair amount of peer pressure to pair but if the pair don't know what they can get out of it, it's unlikely to succeed. We should be honest about that. What makes a good pair (see a <a href="/blog/2008/12/31/what-makes-good-pair/">previous post</a>) and how do we know that we're getting something out of it?</p>

<!-- more -->


<p>Once we honest in our experiences and expectations around pairing, we were able to be explicit in what we want to achieve. We had the conversation along the lines of "do we even believe we <em>could</em> get something out of pairing? Do we want to try?" and when the team bought into that idea we could be explicit about starting from scratch and putting tools in place to help us.</p>

<p>Some techniques I favour include</p>

<ul>
<li><p>When sitting down to start a session, state explicitly what each of you hopes to get out of the session. In short, what you think a pairing session should be like.</p></li>
<li><p>Each pair can set up one or two "rules", written on cards and placed prominently to remind each other about something they feel is important to the session. For example, I often have a rule "ask don't tell" to remind me to be asked before I go rambling on about something in the code.</p></li>
<li><p>The silent partner; saying up front that when not driving, the silent partner should hold their tongue and make notes / record a task stack. Interruption is interruption, so be considerate about when to make a comment. This one should be agreed up front, it may or may not be appropriate for your pair.</p></li>
<li><p>Rotate the pairs frequently. You'll have to agree amongst yourselves at what frequency but I think around two days is enough before rotating.</p></li>
<li><p><strong>Most important one of all</strong>; have a mini pair-retrospective at the end of the session. It's a great chance to ask the question "how was that for you?" and a great way to let the other person know if some aspect didn't go well.</p></li>
</ul>


<p>Another useful tool can be the pair stairs. It's very easy to avoid pairing rather than face it's challenges so <a href="http://www.natpryce.com/articles/000522.html">pair stairs</a> can be a good way to keep you honest. Probably the most helpful thing though is having people on the team that have lots of experience pairing, know what the team can get out of it and are able to help guide pairing sessions for the less experienced.</p>

<p>Pairing is hard. It's probably one of the hardest things we do as developers but it can also be one of the most rewarding, personally and for the team as a whole. Collaboration has given the world some of its greatest achievements but you have to be honest that in this case, you might have to work for it.</p>
]]></content>
  </entry>
  
</feed>
